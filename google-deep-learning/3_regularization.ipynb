{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import copy\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "nx = train_dataset.shape[1]\n",
    "ny = train_labels.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    \"\"\"\n",
    "    A classifier object.\n",
    "    \n",
    "    nx = number of features for the dataset (size of input layer)\n",
    "    ny = number of features for the output (number of classes)\n",
    "    batch_size = size per batch\n",
    "    epochs = number of epochs to train for\n",
    "    print_loss_interval = how often losses are printed to the screen during training, defined as # of training steps\n",
    "    max_iter = maximum number of iterations to train for\n",
    "    learning_rate = the learning rate alpha\n",
    "    reg_lambda = the L2 regularization parameter lambda\n",
    "    name = the name for this classifier, which is used as an unique identifier in storing the trained parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, nx, ny, layers = [], batch_size = 128, epochs = 2, print_loss_interval = 500, max_iter = None,\n",
    "                 learning_rate = 0.001, learn_decay_rate = 0, learn_decay_steps = 5000,\n",
    "                 reg_lambda = 0.5, dropout_rate = 0, name = 'classifier'):\n",
    "        self.name = name\n",
    "        self.nx = nx\n",
    "        self.ny = ny\n",
    "        self.savefile = './models/' + ''.join(c for c in name if c.isalnum() or c in ['_', '-']) + '.model'\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learn_decay_rate = learn_decay_rate\n",
    "        self.learn_decay_steps = learn_decay_steps\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.print_loss_interval = print_loss_interval\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.tf_train_X = tf.placeholder(tf.float32, shape=(None, nx))\n",
    "            self.tf_train_Y = tf.placeholder(tf.float32, shape=(None, ny))\n",
    "            self.tf_valid_X = tf.placeholder(tf.float32, shape=(None, nx))\n",
    "            m = tf.to_float(tf.shape(self.tf_train_X)[0])\n",
    "\n",
    "            # Initialize weights and biases variables\n",
    "            self.all_layers = copy.deepcopy(self.layers)\n",
    "            self.all_layers.insert(0, nx)\n",
    "            self.all_layers.append(ny)\n",
    "            self.weights = [None] * len(self.all_layers)\n",
    "            self.biases = [None] * len(self.all_layers)\n",
    "            for i in range(1, len(self.all_layers)):\n",
    "                self.weights[i] = tf.Variable(tf.random_normal((self.all_layers[i - 1], self.all_layers[i]), stddev = np.sqrt(2.0 / self.all_layers[i - 1])),\n",
    "                                             name = 'W' + str(i))\n",
    "                self.biases[i] = tf.Variable(tf.zeros((self.all_layers[i])), name = 'b' + str(i))\n",
    "\n",
    "            # Training computation.\n",
    "            self.logits = self.__forward_prop(self.tf_train_X)\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.tf_train_Y)) + (reg_lambda / (2 * m)) * tf.add_n([tf.nn.l2_loss(w) for w in self.weights[1:]])\n",
    "            \n",
    "            # Optimizer.\n",
    "            if learn_decay_rate > 0 and learn_decay_steps > 0:\n",
    "                global_step = tf.Variable(0, name = 'global_step', trainable = False)\n",
    "                self.learn_rate_decayed = tf.train.exponential_decay(self.learning_rate, global_step, learn_decay_steps, learn_decay_rate)\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learn_rate_decayed).minimize(self.loss, global_step = global_step)\n",
    "            else:            \n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "            # Predictor.\n",
    "            self.predictor = tf.nn.softmax(self.logits)\n",
    "            self.valid_predictor = tf.nn.softmax(self.__forward_prop(self.tf_valid_X, is_train = False))\n",
    "            \n",
    "            # Saver.\n",
    "            self.saver = tf.train.Saver()\n",
    "    \n",
    "    def __forward_prop(self, X, is_train = True):\n",
    "        with self.graph.as_default():\n",
    "            Z = [None] * len(self.all_layers)\n",
    "            A = [None] * len(self.all_layers)\n",
    "            A[0] = X\n",
    "\n",
    "            for i in range(1, len(self.all_layers)):\n",
    "                Z[i] = tf.matmul(A[i - 1], self.weights[i]) + self.biases[i]\n",
    "                if i != len(self.all_layers) - 1:\n",
    "                    if is_train and self.dropout_rate > 0:\n",
    "                        A[i] = tf.nn.dropout(tf.nn.relu(Z[i]), 1 - self.dropout_rate)\n",
    "                    else:\n",
    "                        A[i] = tf.nn.relu(Z[i])\n",
    "            \n",
    "            return Z[-1]\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        with tf.Session(graph=self.graph) as session:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(session, self.savefile)\n",
    "            p = tf.nn.softmax(self.__forward_prop(tf.constant(X, dtype=tf.float32), is_train = False)).eval()\n",
    "            return p\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_probs(X), 1)\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        return (100.0 * np.sum(predictions == np.argmax(Y, 1)) / predictions.shape[0])\n",
    "    \n",
    "    def train(self, train_X, train_Y, valid_X = None, valid_Y = None):\n",
    "        m = train_X.shape[0]\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print(\"Initialized!\")\n",
    "            step = 0\n",
    "\n",
    "            num_batches = int(math.floor(m / self.batch_size))\n",
    "            if m % self.batch_size != 0:\n",
    "                num_batches += 1\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "                gc.collect(); indices = np.arange(m); np.random.shuffle(indices)\n",
    "                for i in range(num_batches):\n",
    "                    step += 1\n",
    "\n",
    "                    if i == num_batches - 1:\n",
    "                        batch_X = train_X[indices[self.batch_size * i:m]]\n",
    "                        batch_Y = train_Y[indices[self.batch_size * i:m]]\n",
    "                    else:\n",
    "                        batch_X = train_X[indices[self.batch_size * i:self.batch_size * (i + 1)]]\n",
    "                        batch_Y = train_Y[indices[self.batch_size * i:self.batch_size * (i + 1)]]\n",
    "                    \n",
    "                    _, l, predictions= session.run([self.optimizer, self.loss, self.predictor], \n",
    "                                                    feed_dict={self.tf_train_X: batch_X, self.tf_train_Y: batch_Y})                    \n",
    "                    \n",
    "                    if (step % self.print_loss_interval == 0):\n",
    "                        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                        print(\"Minibatch accuracy: %.1f%%\" % \n",
    "                              (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(batch_Y, 1)) / predictions.shape[0]))\n",
    "                        \n",
    "                        if valid_X is not None and valid_Y is not None:\n",
    "                            valid_predictions = session.run(self.valid_predictor, feed_dict={self.tf_valid_X: valid_X})\n",
    "                            print(\"Validation accuracy: %.1f%%\" %\n",
    "                                  (100.0 * np.sum(np.argmax(valid_predictions, 1) == np.argmax(valid_Y, 1)) / valid_Y.shape[0]))\n",
    "                        \n",
    "                        # Save weights for this step\n",
    "                        self.saver.save(session, self.savefile)\n",
    "                    \n",
    "                    # Print new learning rate\n",
    "                    if self.learn_decay_rate > 0 and self.learn_decay_steps > 0 and step % self.learn_decay_steps == 0:\n",
    "                        print(\"New learning rate:\", self.learn_rate_decayed.eval())\n",
    "                    \n",
    "                    if self.max_iter and step >= self.max_iter:\n",
    "                        # Save weights and biases early because there was a hard cap on iterations\n",
    "                        self.saver.save(session, self.savefile)\n",
    "                        return\n",
    "\n",
    "            # Save weights and biases\n",
    "            self.saver.save(session, self.savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "***********************************************\n",
      "Minibatch loss at step 500: 3.782097\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 63.5%\n",
      "Minibatch loss at step 1000: 2.666139\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 1500: 3.117322\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 72.9%\n",
      "***********************************************\n",
      "Minibatch loss at step 2000: 2.023502\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 2500: 2.276888\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 3000: 1.963044\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 75.0%\n",
      "***********************************************\n",
      "Validation accuracy: 74.9%\n",
      "Test accuracy: 82.7%\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = Classifier(nx, ny, reg_lambda = 0, name = \"logistic_regression\")\n",
    "logistic_regression.train(train_dataset, train_labels, valid_dataset, valid_labels)\n",
    "print(\"***********************************************\")\n",
    "print(\"Validation accuracy: %.1f%%\" % logistic_regression.score(valid_dataset, valid_labels))\n",
    "print(\"Test accuracy: %.1f%%\" % logistic_regression.score(test_dataset, test_labels))\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "***********************************************\n",
      "Minibatch loss at step 500: 1433049.750000\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 64.1%\n",
      "Minibatch loss at step 1000: 561720.750000\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 56.8%\n",
      "Minibatch loss at step 1500: 241709.468750\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 61.9%\n",
      "***********************************************\n",
      "Minibatch loss at step 2000: 77499.109375\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 61.3%\n",
      "Minibatch loss at step 2500: 119174.976562\n",
      "Minibatch accuracy: 57.0%\n",
      "Validation accuracy: 63.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c6ff31b5f10e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m one_layer_nn = Classifier(nx, ny, layers = [1024], reg_lambda = 0, print_loss_interval = 500, \n\u001b[1;32m      2\u001b[0m                           learning_rate = 10, name = \"one_layer_nn\")\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mone_layer_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***********************************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation accuracy: %.1f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mone_layer_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4f56b98c6160>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_X, train_Y, valid_X, valid_Y)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     _, l, predictions = session.run([self.optimizer, self.loss, self.predictor], \n\u001b[0;32m--> 118\u001b[0;31m                                                     feed_dict={self.tf_train_X: batch_X, self.tf_train_Y: batch_Y})                    \n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_loss_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "one_layer_nn = Classifier(nx, ny, layers = [1024], reg_lambda = 0, print_loss_interval = 500, \n",
    "                          print_loss_interval = 100, learning_rate = 10, name = \"one_layer_nn\")\n",
    "one_layer_nn.train(train_dataset, train_labels, valid_dataset, valid_labels)\n",
    "print(\"***********************************************\")\n",
    "print(\"Validation accuracy: %.1f%%\" % one_layer_nn.score(valid_dataset, valid_labels))\n",
    "print(\"Test accuracy: %.1f%%\" % one_layer_nn.score(test_dataset, test_labels))\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "***********************************************\n",
      "Minibatch loss at step 500: 10.195614\n",
      "Minibatch accuracy: 61.7%\n",
      "Minibatch loss at step 1000: 8.546135\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 1500: 5.837636\n",
      "Minibatch accuracy: 78.1%\n",
      "***********************************************\n",
      "Minibatch loss at step 2000: 5.418159\n",
      "Minibatch accuracy: 77.3%\n",
      "Minibatch loss at step 2500: 4.613855\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 3000: 3.016995\n",
      "Minibatch accuracy: 79.7%\n",
      "***********************************************\n",
      "Validation accuracy: 77.2%\n",
      "Test accuracy: 84.5%\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "log_regression_reg = Classifier(nx, ny, reg_lambda = 0.5, name = \"log_regression_reg\")\n",
    "log_regression_reg.train(train_dataset, train_labels, valid_dataset, valid_labels)\n",
    "print(\"***********************************************\")\n",
    "print(\"Validation accuracy: %.1f%%\" % log_regression_reg.score(valid_dataset, valid_labels))\n",
    "print(\"Test accuracy: %.1f%%\" % log_regression_reg.score(test_dataset, test_labels))\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "***********************************************\n",
      "Minibatch loss at step 100: 713.626343\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 72.4%\n",
      "Minibatch loss at step 200: 553.274231\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 300: 443.774170\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 400: 360.493225\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 500: 295.724182\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 600: 245.270111\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 700: 198.270386\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 800: 165.640488\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 900: 134.485794\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 1000: 109.733940\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1100: 91.057167\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1200: 74.409119\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1300: 61.332294\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 1400: 50.236610\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 1500: 41.488426\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.2%\n",
      "***********************************************\n",
      "Minibatch loss at step 1600: 34.001163\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1700: 27.997078\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1800: 23.316605\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 1900: 19.178459\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 2000: 15.893621\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 2100: 13.137547\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 2200: 11.005081\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 2300: 9.015685\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 2400: 7.577055\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 2500: 6.333372\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2600: 5.380245\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 2700: 4.487538\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 2800: 3.791557\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 2900: 3.143924\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 3000: 2.658211\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 3100: 2.554691\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 86.7%\n",
      "***********************************************\n",
      "Minibatch loss at step 3200: 2.132934\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 3300: 1.621532\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 3400: 1.556104\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 3500: 1.360791\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 3600: 1.267360\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 3700: 1.088979\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 3800: 0.999432\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 3900: 0.770334\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 4000: 0.919901\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 4100: 0.849110\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 4200: 0.777520\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 4300: 0.652516\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 4400: 0.720853\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 4500: 0.797594\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 4600: 0.598132\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.2%\n",
      "***********************************************\n",
      "Minibatch loss at step 4700: 0.659791\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 4800: 0.570935\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 4900: 0.683651\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 5000: 0.507410\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5100: 0.703058\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 5200: 0.538154\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 5300: 0.683915\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 5400: 0.614096\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 5500: 0.428593\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5600: 0.551625\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 5700: 0.737780\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 5800: 0.528420\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 5900: 0.595961\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 6000: 0.598740\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 6100: 0.445447\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 6200: 0.521468\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.2%\n",
      "***********************************************\n",
      "Minibatch loss at step 6300: 0.701631\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 6400: 0.696198\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 6500: 0.523412\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 6600: 0.763240\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 6700: 0.615176\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 6800: 0.560078\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 6900: 0.495547\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 7000: 0.389832\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 7100: 0.586523\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 7200: 0.456821\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 7300: 0.525604\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 7400: 0.569090\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 7500: 0.676713\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 7600: 0.509736\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 7700: 0.441481\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 7800: 0.564180\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.6%\n",
      "***********************************************\n",
      "Validation accuracy: 87.1%\n",
      "Test accuracy: 93.1%\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "one_layer_nn_reg = Classifier(nx, ny, layers = [1024], reg_lambda = 0.5, name = \"one_layer_nn_reg\",\n",
    "                             print_loss_interval = 100, epochs = 5)\n",
    "one_layer_nn_reg.train(train_dataset, train_labels, valid_dataset, valid_labels)\n",
    "print(\"***********************************************\")\n",
    "print(\"Validation accuracy: %.1f%%\" % one_layer_nn_reg.score(valid_dataset, valid_labels))\n",
    "print(\"Test accuracy: %.1f%%\" % one_layer_nn_reg.score(test_dataset, test_labels))\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "***********************************************\n",
      "Minibatch loss at step 40: 41.842270\n",
      "Minibatch accuracy: 62.5%\n",
      "***********************************************\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "***********************************************\n",
      "Minibatch loss at step 120: 44.864929\n",
      "Minibatch accuracy: 50.0%\n",
      "***********************************************\n",
      "Minibatch loss at step 160: 47.206039\n",
      "Minibatch accuracy: 87.5%\n",
      "***********************************************\n",
      "Minibatch loss at step 200: 3.977848\n",
      "Minibatch accuracy: 87.5%\n",
      "***********************************************\n",
      "Validation accuracy: 69.3%\n",
      "Test accuracy: 76.2%\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "restricted_train_dataset = train_dataset[:5000]\n",
    "restricted_train_labels = train_labels[:5000]\n",
    "overfit_one_layer_nn = Classifier(nx, ny, layers = [1024], reg_lambda = 0, epochs = 5,\n",
    "                                  print_loss_interval = 40, name = \"overfit_one_layer_nn\")\n",
    "overfit_one_layer_nn.train(restricted_train_dataset, restricted_train_labels)\n",
    "print(\"***********************************************\")\n",
    "print(\"Validation accuracy: %.1f%%\" % overfit_one_layer_nn.score(valid_dataset, valid_labels))\n",
    "print(\"Test accuracy: %.1f%%\" % overfit_one_layer_nn.score(test_dataset, test_labels))\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "***********************************************\n",
      "Minibatch loss at step 500: 575.905273\n",
      "Minibatch accuracy: 85.2%\n",
      "Minibatch loss at step 1000: 515.922791\n",
      "Minibatch accuracy: 76.6%\n",
      "Minibatch loss at step 1500: 446.372192\n",
      "Minibatch accuracy: 79.7%\n",
      "***********************************************\n",
      "Minibatch loss at step 2000: 394.030212\n",
      "Minibatch accuracy: 78.9%\n",
      "Minibatch loss at step 2500: 335.208923\n",
      "Minibatch accuracy: 78.9%\n",
      "Minibatch loss at step 3000: 290.856323\n",
      "Minibatch accuracy: 84.4%\n",
      "***********************************************\n",
      "Validation accuracy: 83.4%\n",
      "Test accuracy: 89.8%\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "one_layer_nn_dropout = Classifier(nx, ny, layers = [1024], reg_lambda = 0.5, \n",
    "                                  epochs = 2, name = \"one_layer_nn_dropout3\")\n",
    "one_layer_nn_dropout.train(train_dataset, train_labels)\n",
    "print(\"***********************************************\")\n",
    "print(\"Validation accuracy: %.1f%%\" % one_layer_nn_dropout.score(valid_dataset, valid_labels))\n",
    "print(\"Test accuracy: %.1f%%\" % one_layer_nn_dropout.score(test_dataset, test_labels))\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "INFO:tensorflow:Restoring parameters from ./models/best_nn.model\n",
      "Validation accuracy: 87.9%\n",
      "INFO:tensorflow:Restoring parameters from ./models/best_nn.model\n",
      "Test accuracy: 94.3%\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "best_nn = Classifier(nx, ny, layers = [1024, 256], reg_lambda = 0.5, batch_size = 128, learning_rate = 0.001,\n",
    "                     print_loss_interval = 100, epochs = 10, name = \"best_nn\")\n",
    "#best_nn.train(train_dataset, train_labels, valid_dataset, valid_labels)\n",
    "print(\"***********************************************\")\n",
    "print(\"Validation accuracy: %.1f%%\" % best_nn.score(valid_dataset, valid_labels))\n",
    "print(\"Test accuracy: %.1f%%\" % best_nn.score(test_dataset, test_labels))\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "INFO:tensorflow:Restoring parameters from ./models/best_nn2.model\n",
      "Validation accuracy: 89.4%\n",
      "INFO:tensorflow:Restoring parameters from ./models/best_nn2.model\n",
      "Test accuracy: 95.2%\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "best_nn2 = Classifier(nx, ny, layers = [1024, 256], reg_lambda = 0.5, batch_size = 128, learning_rate = 0.001,\n",
    "                      learn_decay_rate = 0.6, learn_decay_steps = 2000, print_loss_interval = 100, \n",
    "                      epochs = 15, name = \"best_nn2\")\n",
    "#best_nn2.train(train_dataset, train_labels, valid_dataset, valid_labels)\n",
    "print(\"***********************************************\")\n",
    "print(\"Validation accuracy: %.1f%%\" % best_nn2.score(valid_dataset, valid_labels))\n",
    "print(\"Test accuracy: %.1f%%\" % best_nn2.score(test_dataset, test_labels))\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "INFO:tensorflow:Restoring parameters from ./models/best_nn3.model\n",
      "Validation accuracy: 89.6%\n",
      "INFO:tensorflow:Restoring parameters from ./models/best_nn3.model\n",
      "Test accuracy: 95.3%\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "best_nn3 = Classifier(nx, ny, layers = [1024, 256], reg_lambda = 0.5, batch_size = 128, learning_rate = 0.001,\n",
    "                      learn_decay_rate = 0.75, learn_decay_steps = 2000, print_loss_interval = 1000, \n",
    "                      epochs = 15, name = \"best_nn3\")\n",
    "#best_nn3.train(train_dataset, train_labels, valid_dataset, valid_labels)\n",
    "print(\"***********************************************\")\n",
    "print(\"Validation accuracy: %.1f%%\" % best_nn3.score(valid_dataset, valid_labels))\n",
    "print(\"Test accuracy: %.1f%%\" % best_nn3.score(test_dataset, test_labels))\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "INFO:tensorflow:Restoring parameters from ./models/best_nn4.model\n",
      "Validation accuracy: 89.0%\n",
      "INFO:tensorflow:Restoring parameters from ./models/best_nn4.model\n",
      "Test accuracy: 95.1%\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "best_nn4 = Classifier(nx, ny, layers = [1024, 256], reg_lambda = 0.5, batch_size = 128, learning_rate = 0.001,\n",
    "                      learn_decay_rate = 0.8, learn_decay_steps = 3000, print_loss_interval = 1000, \n",
    "                      epochs = 15, name = \"best_nn4\")\n",
    "#best_nn4.train(train_dataset, train_labels, valid_dataset, valid_labels)\n",
    "print(\"***********************************************\")\n",
    "print(\"Validation accuracy: %.1f%%\" % best_nn4.score(valid_dataset, valid_labels))\n",
    "print(\"Test accuracy: %.1f%%\" % best_nn4.score(test_dataset, test_labels))\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
